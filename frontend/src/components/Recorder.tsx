'use client';

import { useState } from 'react';
import { Mic, Square, Pause, Play, Loader2 } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { useRecorder } from '@/hooks/useRecorder';
import { CostDisplay } from '@/components/CostDisplay';
import { formatDuration } from '@/lib/utils';
import { CostBreakdown, TokenUsage } from '@/types';
import { calculateTotalCost } from '@/lib/costCalculator';

export const Recorder = () => {
  const {
    recordingState,
    startRecording,
    stopRecording,
    pauseRecording,
    resumeRecording,
  } = useRecorder();

  const [isProcessing, setIsProcessing] = useState(false);
  const [processingStep, setProcessingStep] = useState<string>('');
  const [costBreakdown, setCostBreakdown] = useState<CostBreakdown | null>(null);
  const [tokenUsage, setTokenUsage] = useState<TokenUsage | null>(null);
  const [showCostDisplay, setShowCostDisplay] = useState(false);

  const handleStartRecording = async () => {
    try {
      await startRecording();
      // Reset cost display when starting new recording
      setCostBreakdown(null);
      setTokenUsage(null);
      setShowCostDisplay(false);
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const handleStopRecording = async () => {
    try {
      const audioBlob = await stopRecording();
      if (audioBlob) {
        await processRecording(audioBlob, recordingState.duration / 1000);
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const processRecording = async (audioBlob: Blob, durationSeconds: number) => {
    setIsProcessing(true);
    
    try {
      // Simulate transcription step
      setProcessingStep('Transcribing audio...');
      await new Promise(resolve => setTimeout(resolve, 2000));
      
      // Simulate transcript text (in real app, this would come from Whisper API)
      const mockTranscript = `This is a simulated transcript of a ${Math.round(durationSeconds)} second recording. It contains sample text that would normally be generated by OpenAI's Whisper API. The content would include whatever was spoken during the recording session.`;
      
      // Simulate summarization step
      setProcessingStep('Generating summary...');
      await new Promise(resolve => setTimeout(resolve, 1500));
      
      // Calculate costs
      const costEstimate = calculateTotalCost(durationSeconds, mockTranscript);
      setCostBreakdown(costEstimate);
      setTokenUsage({
        transcription: costEstimate.transcription.tokens,
        summarization: costEstimate.summarization.tokens,
        total: costEstimate.total.tokens,
      });
      
      setProcessingStep('Processing complete!');
      setShowCostDisplay(true);
      
      // Hide processing message after a delay
      setTimeout(() => {
        setIsProcessing(false);
        setProcessingStep('');
      }, 2000);
      
    } catch (error) {
      console.error('Failed to process recording:', error);
      setProcessingStep('Processing failed');
      setTimeout(() => {
        setIsProcessing(false);
        setProcessingStep('');
      }, 2000);
    }
  };

  const handlePauseResume = () => {
    if (recordingState.isPaused) {
      resumeRecording();
    } else {
      pauseRecording();
    }
  };

  return (
    <div className="flex flex-col items-center space-y-4 p-6 bg-white rounded-lg shadow-md">
      <div className="text-2xl font-semibold text-gray-800">
        {isProcessing ? 'Processing...' : recordingState.isRecording ? 'Recording...' : 'Ready to Record'}
      </div>
      
      {isProcessing && (
        <div className="flex items-center gap-2 text-lg text-blue-600">
          <Loader2 className="w-5 h-5 animate-spin" />
          <span>{processingStep}</span>
        </div>
      )}
      
      {recordingState.isRecording && !isProcessing && (
        <div className="text-lg text-gray-600">
          {formatDuration(recordingState.duration)}
        </div>
      )}

      <div className="flex space-x-4">
        {!recordingState.isRecording && !isProcessing ? (
          <Button
            onClick={handleStartRecording}
            size="lg"
            className="bg-red-500 hover:bg-red-600"
          >
            <Mic className="w-5 h-5 mr-2" />
            Start Recording
          </Button>
        ) : recordingState.isRecording && !isProcessing ? (
          <>
            <Button
              onClick={handlePauseResume}
              variant="outline"
              size="lg"
            >
              {recordingState.isPaused ? (
                <>
                  <Play className="w-5 h-5 mr-2" />
                  Resume
                </>
              ) : (
                <>
                  <Pause className="w-5 h-5 mr-2" />
                  Pause
                </>
              )}
            </Button>
            
            <Button
              onClick={handleStopRecording}
              size="lg"
              className="bg-gray-500 hover:bg-gray-600"
            >
              <Square className="w-5 h-5 mr-2" />
              Stop Recording
            </Button>
          </>
        ) : (
          <div className="text-sm text-gray-500">
            Please wait while we process your recording...
          </div>
        )}
      </div>

      {/* Cost Display */}
      {costBreakdown && tokenUsage && (
        <CostDisplay
          costBreakdown={costBreakdown}
          tokenUsage={tokenUsage}
          isVisible={showCostDisplay}
          onClose={() => setShowCostDisplay(false)}
        />
      )}
    </div>
  );
}; 